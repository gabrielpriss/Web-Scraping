# Web Scraping com Python, projeto feito durante o curso da Trybe

<details>
  <summary><strong>Objetivo</strong></summary><br />

  O projeto tem como principal objetivo fazer consultas em not√≠cias sobre tecnologia.

  As not√≠cias raspadas est√£o no Blog da Trybe: https://blog.betrybe.com.
  Essas not√≠cias s√£o salvas no banco de dados utilizando as fun√ß√µes python no m√≥dulo `database.py`
  
  ---

  <strong>MongoDB</strong>

  √â utilizado um banco de dados chamado `tech_news`.
  As not√≠cias s√£o armazenadas em uma cole√ß√£o chamada `news`.

  Para rodar o MongoDB via Docker:
  <code>docker-compose up -d mongodb</code> no terminal. 
  Configura√ß√µes do mongo com o docker est√£o no arquivo `docker-compose.yml`

  O mongoDB utilizar√° por padr√£o a porta 27017.

</details>

---
<details>
  <summary><strong> Requisitos</strong></summary>

  ## 1 - Fun√ß√£o `fetch`
  local: `tech_news/scraper.py`

  Esta fun√ß√£o √© respons√°vel por fazer a requisi√ß√£o HTTP ao site e obter o conte√∫do HTML.

  - Utiliza um Rate Limit pois pode ser utilizada v√°rias vezes em sucess√£o
  - Deve receber uma URL
  - Faz uma requisi√ß√£o HTTP `get` para a URL utilizando a fun√ß√£o `requests.get`
  - Retorna o conte√∫do HTML da resposta.
  - Caso a requisi√ß√£o seja bem sucedida com `Status Code 200: OK`, retorna seu conte√∫do de texto;
  - Caso a resposta tenha o c√≥digo de status diferente de `200`, retorna `None`;
  - Caso a requisi√ß√£o n√£o receba resposta em at√© 3 segundos, ela √© abandonada.

  üìå √© definido o _header_ `user-agent` para que a raspagem do blog funcione corretamente. Para isso, o valor `"Fake user-agent"` recebe:

  ```python
  { "user-agent": "Fake user-agent" }
  ```

  ## 2 - Fun√ß√£o `scrape_novidades`
  local: `tech_news/scraper.py`

  Esta fun√ß√£o faz o scrape da p√°gina Novidades (https://blog.betrybe.com) para obter as URLs das p√°ginas de not√≠cias.

  - Recebe uma string com o conte√∫do HTML da p√°gina inicial do blog
  - Faz o scrape do conte√∫do recebido para obter uma lista contendo as URLs das not√≠cias listadas.
  - A fun√ß√£o retorna esta lista.
  - Caso n√£o encontre nenhuma URL de not√≠cia, a fun√ß√£o retorna uma lista vazia.

  ## 3 - Fun√ß√£o `scrape_next_page_link`
  local: `tech_news/scraper.py`

  Precisa do link da pr√≥xima p√°gina. Esta fun√ß√£o √© respons√°vel por fazer o scrape deste link.

  - Recebe como par√¢metro uma `string` contendo o conte√∫do HTML retirado pela fun√ß√£o fetch
  - Faz o scrape deste HTML para obter a URL da pr√≥xima p√°gina.
  - Retorna a URL obtida.
  - Caso n√£o encontre o link da pr√≥xima p√°gina, fun√ß√£o retorna `None`

  ## 4 - Fun√ß√£o `scrape_noticia`
  local: `tech_news/scraper.py`

  - Recebe como par√¢metro o conte√∫do HTML da p√°gina de uma not√≠cia
  - Busca as informa√ß√µes das not√≠cias e preenche um dicion√°rio:
    - `url` - link para acesso da not√≠cia.
    - `title` - t√≠tulo da not√≠cia.
    - `timestamp` - data da not√≠cia, no formato `dd/mm/AAAA`.
    - `writer` - nome da pessoa autora da not√≠cia.
    - `comments_count` - n√∫mero de coment√°rios que a not√≠cia recebeu.
      - Se a informa√ß√£o n√£o for encontrada, salve este atributo como `0` (zero)
    - `summary` - o primeiro par√°grafo da not√≠cia.
    - `tags` - lista contendo tags da not√≠cia.
    - `category` - categoria da not√≠cia.

  ## 5 - Fun√ß√£o `get_tech_news`
  local: `tech_news/scraper.py`

  Aplica√ß√£o de todas as fun√ß√µes anterioes.

  - Receber como par√¢metro um n√∫mero inteiro `n` e buscar as √∫ltimas `n` not√≠cias.
  - Fun√ß√µes `fetch`, `scrape_novidades`, `scrape_next_page_link` e `scrape_noticia` s√£o utilizadas para buscar as not√≠cias e processar o conte√∫do.
  - As not√≠cias buscadas s√£o inseridas no MongoDB; utiliza as fun√ß√µes do diret√≥rio `tech_news/database.py`
  - Insere as not√≠cias no banco, e retorna as mesmas.


  ## 6 - Fun√ß√£o `search_by_title`
  local: `tech_news/analyzer/search_engine.py`

  Faz buscas por t√≠tulo.

  - Recebe uma string com um t√≠tulo de not√≠cia
  - Busca as not√≠cias do banco de dados por t√≠tulo
  - Retorna uma lista de tuplas com as not√≠cias encontradas na busca. 
  Exemplo: 
  ```python
  [
    ("T√≠tulo1_aqui", "url1_aqui"),
    ("T√≠tulo2_aqui", "url2_aqui"),
  ]
  ```
  - A busca √© _case insensitive_
  - Caso nenhuma not√≠cia seja encontrada, retorna uma lista vazia.

  üìå Para acesso ao banco de dados utiliza `db` definido no m√≥dulo `tech_news/database.py`.

  ## 7 - Fun√ß√£o `search_by_date`
  local: `tech_news/analyzer/search_engine.py`

  Busca as not√≠cias do banco de dados por data.

  - Recebe como par√¢metro uma data no formato ISO `AAAA-mm-dd`
  - Tem retorno no mesmo formato do requisito anterior.
  - Caso a data seja inv√°lida, uma exce√ß√£o `ValueError` √© lan√ßada com a mensagem `Data inv√°lida`.
  - Caso nenhuma not√≠cia seja encontrada, retorna uma lista vazia.

  ## 8 - Fun√ß√£o `search_by_tag`,
  local: `tech_news/analyzer/search_engine.py`

  Busca as not√≠cias por tag.

  - Recebe como par√¢metro o nome da tag completo..
  - A fun√ß√£o deve ter retorno no mesmo formato do requisito anterior.
  - Caso nenhuma not√≠cia seja encontrada, retorna uma lista vazia.
  - A busca √©_case insensitive_

  ## 9 - Fun√ß√£o `search_by_category`
  local: `tech_news/analyzer/search_engine.py`

  Busca as not√≠cias por categoria.

  - Recebe como par√¢metro o nome da categoria completo.
  - Tem como retorno o mesmo formato do requisito anterior.
  - Caso nenhuma not√≠cia seja encontrada, retorna uma lista vazia.
  - A busca √© _case insensitive_
</details>
